#xti configuration
hidden_size: 768
num_hidden_layers: 4
num_attention_heads: 4
intermediate_size: 3072
hidden_act: "gelu"
hidden_dropout_prob: 0.1
attention_probs_dropout_prob: 0.1
initializer_range: 0.02
layer_norm_eps: 1e-12
is_encoder_decoder: False
image_size: 128
patch_size: 16
num_channels: 3
pos_encoding_type: 'learned'
classifier: 'token'
num_classes: 10
